{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1503e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeasted90\u001b[0m (\u001b[33mbeasted90-comudel\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from func_utils.plot_utils import show_image\n",
    "import matplotlib.pyplot as plt \n",
    "from glob import glob\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "import os \n",
    "\n",
    "import torch \n",
    "from func_utils.pydataloader import SynthDogDataset\n",
    "from func_utils.trainer_utils import *\n",
    "from encoder_decoder_model import init_dit_t5_models_fixed\n",
    "\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac1eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model as is is holding: 0.00 of GPU RAM\n"
     ]
    }
   ],
   "source": [
    "def get_synth_images_json_path(data_root= os.path.join('synthdog','outputs'), split='train'):\n",
    "    ipath = os.path.join(data_root, '*', split, '*.jpg')\n",
    "    json_path = os.path.join(data_root, '*', split, 'metadata.jsonl')\n",
    "\n",
    "    return glob(ipath), glob(json_path)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "root_path = os.path.join('synthdog', 'outputs_ol')\n",
    "train_ipath, train_json_metadata = get_synth_images_json_path(data_root=root_path, split='train')\n",
    "val_ipath, val_json_metadata = get_synth_images_json_path(data_root=root_path, split='validation')\n",
    "test_ipath, test_json_metadata = get_synth_images_json_path(data_root=root_path, split='test')\n",
    "processor, text_tokenizer, _ = init_dit_t5_models_fixed()\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "peak_mem = torch.cuda.max_memory_allocated()\n",
    "print(f\"The model as is is holding: {peak_mem / 1024**3:.2f} of GPU RAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a1a924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jaabi\\Documents\\comudel\\ocr\\wandb\\run-20250925_142146-r3lp4dqb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/r3lp4dqb' target=\"_blank\">dit_t5_raw_full_model_unfreeze</a></strong> to <a href='https://wandb.ai/beasted90-comudel/ocr%20model%20' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/beasted90-comudel/ocr%20model%20' target=\"_blank\">https://wandb.ai/beasted90-comudel/ocr%20model%20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/r3lp4dqb' target=\"_blank\">https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/r3lp4dqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/r3lp4dqb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1a76318ddd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name = \"dit_t5_raw_full_model_unfreeze\"\n",
    "wandb.init(project=\"ocr model\", name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed635612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['synthdog\\\\outputs_ol\\\\SynthDoG_en\\\\train\\\\image_0.jpg', 'synthdog\\\\outputs_ol\\\\SynthDoG_en\\\\train\\\\image_1.jpg']\n",
      "Sampled lang counter: {'pt': 500, 'en': 500}\n",
      "Length of _.images: 1000 | Length of _.json_metadata: 32011\n",
      "['synthdog\\\\outputs_ol\\\\SynthDoG_en\\\\validation\\\\image_10007.jpg', 'synthdog\\\\outputs_ol\\\\SynthDoG_en\\\\validation\\\\image_10017.jpg']\n",
      "Sampled lang counter: {'pt': 10, 'en': 10}\n",
      "Length of _.images: 20 | Length of _.json_metadata: 4008\n",
      "['synthdog\\\\outputs_ol\\\\SynthDoG_en\\\\test\\\\image_10.jpg', 'synthdog\\\\outputs_ol\\\\SynthDoG_en\\\\test\\\\image_10003.jpg']\n",
      "Sampled lang counter: {'pt': 10, 'en': 10}\n",
      "Length of _.images: 20 | Length of _.json_metadata: 3978\n"
     ]
    }
   ],
   "source": [
    "max_token_size = 512\n",
    "sample_size = 1000\n",
    "train_synthdataset = SynthDogDataset(train_ipath, train_json_metadata, image_feature_extractor=processor, \n",
    "                                     text_tokenizer=text_tokenizer, max_token_size=max_token_size, sample_size=sample_size)\n",
    "val_synthdataset = SynthDogDataset(val_ipath, val_json_metadata, image_feature_extractor=processor, \n",
    "                                   text_tokenizer=text_tokenizer, max_token_size=max_token_size, sample_size=20)\n",
    "test_synthdataset = SynthDogDataset(test_ipath, test_json_metadata, image_feature_extractor=processor, \n",
    "                                    text_tokenizer=text_tokenizer, max_token_size=max_token_size, sample_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2fb665",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=32\n",
    "alpha=r*2\n",
    "dropout=0.3\n",
    "target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\",\n",
    "        # \"fc1\", \"fc2\"\n",
    "]\n",
    "# modules_to_save = [\"embed_tokens\", \"lm_head\"]\n",
    "modules_to_save = None\n",
    "\n",
    "num_epochs = 500\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"./{run_name}\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=9e-5,  \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        warmup_ratio=0.1,  \n",
    "        logging_steps=50,\n",
    "        # save_steps=50,\n",
    "        # eval_steps=50,\n",
    "        logging_strategy=\"steps\",\n",
    "        save_total_limit=3,\n",
    "        fp16=False,\n",
    "        max_grad_norm=10,  \n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        dataloader_pin_memory=False,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=512,\n",
    "        generation_num_beams=6,\n",
    "        report_to=[\"wandb\"],\n",
    "        run_name=run_name,\n",
    "        save_safetensors=False,\n",
    "\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        load_best_model_at_end=True,  \n",
    "        greater_is_better=False,\n",
    "\n",
    "        # label_smoothing_factor=0.1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3811e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trainable: encoder.embeddings.cls_token\n",
      "✅ Trainable: encoder.embeddings.position_embeddings\n",
      "✅ Trainable: encoder.embeddings.patch_embeddings.projection.weight\n",
      "✅ Trainable: encoder.embeddings.patch_embeddings.projection.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.0.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.1.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.2.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.3.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.4.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.5.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.6.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.7.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.8.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.9.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.10.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.11.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.layernorm_after.bias\n",
      "✅ Trainable: encoder.pooler.layernorm.weight\n",
      "✅ Trainable: encoder.pooler.layernorm.bias\n",
      "✅ Trainable: decoder.shared.weight\n",
      "✅ Trainable: decoder.encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "✅ Trainable: decoder.encoder.block.0.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.0.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.0.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.1.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.1.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.1.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.2.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.2.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.2.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.3.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.3.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.3.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.4.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.4.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.4.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.5.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.5.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.5.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.6.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.6.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.6.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.6.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.6.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.6.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.6.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.6.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.7.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.7.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.7.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.7.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.7.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.7.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.7.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.7.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.8.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.8.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.8.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.8.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.8.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.8.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.8.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.8.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.9.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.9.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.9.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.9.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.9.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.9.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.9.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.9.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.10.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.10.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.10.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.10.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.10.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.10.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.10.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.10.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.11.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.encoder.block.11.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.encoder.block.11.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.encoder.block.11.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.encoder.block.11.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.block.11.layer.1.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.encoder.block.11.layer.1.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.encoder.block.11.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.encoder.final_layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.0.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.1.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.2.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.3.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.4.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.5.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.6.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.7.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.8.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.9.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.10.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.0.SelfAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.0.SelfAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.0.SelfAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.0.SelfAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.0.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.1.EncDecAttention.q.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.1.EncDecAttention.k.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.1.EncDecAttention.v.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.1.EncDecAttention.o.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.1.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.2.DenseReluDense.wi.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.2.DenseReluDense.wo.weight\n",
      "✅ Trainable: decoder.decoder.block.11.layer.2.layer_norm.weight\n",
      "✅ Trainable: decoder.decoder.final_layer_norm.weight\n"
     ]
    }
   ],
   "source": [
    "image_processor, text_tokenizer, ovmodel = init_dit_t5_models_fixed()\n",
    "# ovmodel = add_lora_to_decoder(ovmodel, r=r, alpha=alpha, dropout=dropout, target_modules=target_modules, modules_to_save=modules_to_save)\n",
    "# ovmodel = unfreeze_all_params(ovmodel, unfreeze_encoder=False, unfreeze_decoder=True)\n",
    "# ovmodel = unfreeze_last_n_encoder(ovmodel, unfreeze_last_n_layer_block=1, unfreeze_attention_layers=True,skip_encoder=True, skip_decoder=True)\n",
    "# ovmodel = freeze_encoder_unfreeze_decoder(ovmodel, applied_lora=True)\n",
    "\n",
    "ovmodel.add_cross_attention = True\n",
    "ovmodel.config.max_length = max_token_size\n",
    "ovmodel.config.decoder.max_length = max_token_size\n",
    "ovmodel.config.min_length = 1\n",
    "ovmodel.config.decoder.min_length = 1\n",
    "ovmodel.config.no_repeat_ngram_size = 0\n",
    "ovmodel.config.repetition_penalty = 1.5\n",
    "ovmodel.config.length_penalty = 1.0 \n",
    "ovmodel.config.early_stopping = True\n",
    "ovmodel.config.num_beams = 6\n",
    "ovmodel.config.use_cache = False  \n",
    "ovmodel.config.is_encoder_decoder = True\n",
    "ovmodel.config.do_sample = False  \n",
    "ovmodel.config.tie_word_embeddings = True\n",
    "ovmodel.config.decoder.dropout = dropout\n",
    "ovmodel.config.decoder.attention_dropout = 0.2\n",
    "ovmodel.config.decoder.decoder_layerdrop = 0.15\n",
    "print_trainable_prams(ovmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b1c4919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaabi\\Documents\\comudel\\ocr\\func_utils\\trainer_utils.py:225: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10, \n",
    ")\n",
    "trainer = setup_dit_bart_training(\n",
    "        train_synthdataset, val_synthdataset, training_args=training_args, model=ovmodel, text_tokenizer=text_tokenizer,\n",
    "        run_name = run_name, \n",
    "        callbacks=[early_stopping_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a56d6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__func__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__self__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "tmodel = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tmodel.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af576e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "T5ForConditionalGeneration.forward() got an unexpected keyword argument 'encoder_hidden_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\trainer.py:4009\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4008\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4009\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4011\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4012\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4013\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4014\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4015\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\trainer.py:4099\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4097\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4098\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4099\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4100\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4101\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py:552\u001b[39m, in \u001b[36mVisionEncoderDecoderModel.forward\u001b[39m\u001b[34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[39m\n\u001b[32m    547\u001b[39m     decoder_input_ids = shift_tokens_right(\n\u001b[32m    548\u001b[39m         labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m    549\u001b[39m     )\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[32m    568\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: T5ForConditionalGeneration.forward() got an unexpected keyword argument 'encoder_hidden_states'"
     ]
    }
   ],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "vind = np.random.randint(0, len(train_synthdataset))\n",
    "sample = train_synthdataset[vind]  \n",
    "inputs = sample[\"pixel_values\"].unsqueeze(0).to(model.device)\n",
    "image = sample['image']\n",
    "text = sample['text']\n",
    "output_ids = model.generate(inputs, max_length=100, num_beams=6)\n",
    "prediction = text_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Ground Truth:\", sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb5ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
