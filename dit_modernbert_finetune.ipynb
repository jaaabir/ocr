{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacb0ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeasted90\u001b[0m (\u001b[33mbeasted90-comudel\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "from func_utils.plot_utils import show_image\n",
    "import matplotlib.pyplot as plt \n",
    "from glob import glob\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "import os \n",
    "\n",
    "import torch \n",
    "from func_utils.pydataloader import SynthDogDataset\n",
    "from func_utils.trainer_utils import *\n",
    "from encoder_decoder_model import init_dit_mbert_models_fixed, init_dit_dbart_models, print_model_layer_sizes, load_pretrained_enc_dec_model\n",
    "\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "wandb.login()\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed5fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model as is is holding: 0.00 of GPU RAM\n"
     ]
    }
   ],
   "source": [
    "def get_synth_images_json_path(data_root= os.path.join('synthdog','outputs'), split='train'):\n",
    "    ipath = os.path.join(data_root, '*', split, '*.jpg')\n",
    "    json_path = os.path.join(data_root, '*', split, 'metadata.jsonl')\n",
    "\n",
    "    return glob(ipath), glob(json_path)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "root_path = os.path.join('synthdog', 'outputs_ol')\n",
    "train_ipath, train_json_metadata = get_synth_images_json_path(data_root=root_path, split='train')\n",
    "val_ipath, val_json_metadata = get_synth_images_json_path(data_root=root_path, split='validation')\n",
    "test_ipath, test_json_metadata = get_synth_images_json_path(data_root=root_path, split='test')\n",
    "processor, text_tokenizer = init_dit_dbart_models(load_model=False)\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "peak_mem = torch.cuda.max_memory_allocated()\n",
    "print(f\"The model as is is holding: {peak_mem / 1024**3:.2f} of GPU RAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918f78b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> ÁGUA É ESSENCIAL PARA A COMPREENSÃ O E AÇÃ O; CÂ NCER, ÓRGÃ OS, EMOÇÃ O, TÊ M INFLUÊ NCIA, E ÍNDICES MOSTRAM EVOLUÇÃ O.</s>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'ÁGUA É ESSENCIAL PARA A COMPREENSÃO E AÇÃO; CÂNCER, ÓRGÃOS, EMOÇÃO, TÊM INFLUÊNCIA, E ÍNDICES MOSTRAM EVOLUÇÃO.'\n",
    "text2 = \"água é essencial para a compreensão e ação; câncer, órgãos, emoção, têm influência, e índices mostram evolução.\"\n",
    "\n",
    "text_tokenizer.decode(text_tokenizer(text).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b0d588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jaabi\\Documents\\comudel\\ocr\\wandb\\run-20251001_113607-nvagli72</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/nvagli72' target=\"_blank\">dtesting</a></strong> to <a href='https://wandb.ai/beasted90-comudel/ocr%20model%20' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/beasted90-comudel/ocr%20model%20' target=\"_blank\">https://wandb.ai/beasted90-comudel/ocr%20model%20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/nvagli72' target=\"_blank\">https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/nvagli72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/nvagli72?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1d3d98bb910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name = \"dtesting\"\n",
    "wandb.init(project=\"ocr model\", name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba3dfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['synthdog\\\\outputs_ol\\\\SynthDoG_en\\\\train\\\\image_0.jpg', 'synthdog\\\\outputs_ol\\\\SynthDoG_en\\\\train\\\\image_1.jpg']\n",
      "Sampled lang counter: {'en': 16, 'pt': 16}\n",
      "Length of _.images: 32 | Length of _.json_metadata: 32011\n",
      "['synthdog\\\\outputs_ol\\\\SynthDoG_en\\\\validation\\\\image_10007.jpg', 'synthdog\\\\outputs_ol\\\\SynthDoG_en\\\\validation\\\\image_10017.jpg']\n",
      "Sampled lang counter: {'pt': 2, 'en': 2}\n",
      "Length of _.images: 4 | Length of _.json_metadata: 4008\n"
     ]
    }
   ],
   "source": [
    "max_token_size = 1056\n",
    "sample_size = 32\n",
    "train_synthdataset = SynthDogDataset(train_ipath, train_json_metadata, image_feature_extractor=processor, \n",
    "                                     text_tokenizer=text_tokenizer, max_token_size=max_token_size, sample_size=sample_size)\n",
    "val_synthdataset = SynthDogDataset(val_ipath, val_json_metadata, image_feature_extractor=processor, \n",
    "                                   text_tokenizer=text_tokenizer, max_token_size=max_token_size, sample_size=4)\n",
    "# test_synthdataset = SynthDogDataset(test_ipath, test_json_metadata, image_feature_extractor=processor, \n",
    "#                                     text_tokenizer=text_tokenizer, max_token_size=max_token_size, sample_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50af0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r=32\n",
    "# alpha=r*2\n",
    "# dropout=0.3\n",
    "# target_modules = [\n",
    "#         \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\",\n",
    "# ]\n",
    "# modules_to_save = None\n",
    "\n",
    "num_epochs = 20\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"./{run_name}\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=5e-4,  \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        warmup_ratio=0.1,  \n",
    "        logging_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        save_total_limit=3,\n",
    "        fp16=False,\n",
    "        max_grad_norm=10,  \n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        dataloader_pin_memory=False,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=max_token_size,\n",
    "        generation_num_beams=6,\n",
    "        report_to=[\"wandb\"],\n",
    "        run_name=run_name,\n",
    "        save_safetensors=False,\n",
    "\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        load_best_model_at_end=True,  \n",
    "        greater_is_better=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b232dab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the pre-trained model successfully...\n"
     ]
    }
   ],
   "source": [
    "image_processor, text_tokenizer = init_dit_dbart_models(load_model=False)\n",
    "decoder = \"naver-clova-ix/donut-base\"\n",
    "ckpt_path = 'saved_models\\mydit_dbart'\n",
    "ovmodel = load_pretrained_enc_dec_model(ckpt_path, base_encoder_model=None, \n",
    "                                        base_decoder_model=decoder, \n",
    "                                        lora_applied=False, \n",
    "                                        new_tokens=['Ã', 'Ê', 'Â']\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8ef8eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaTokenizerFast(name_or_path='naver-clova-ix/donut-base', vocab_size=57522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<s_iitcdip>', '<s_synthdog>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57521: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t57522: AddedToken(\"<sep/>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57523: AddedToken(\"<s_iitcdip>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57524: AddedToken(\"<s_synthdog>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57525: AddedToken(\"Ã\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57526: AddedToken(\"Ê\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57527: AddedToken(\"Â\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7913661d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): BeitModel(\n",
       "    (embeddings): BeitEmbeddings(\n",
       "      (patch_embeddings): BeitPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): BeitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): Identity()\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.00909090880304575)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.0181818176060915)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.027272727340459824)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.036363635212183)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.045454543083906174)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.054545458406209946)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.06363636255264282)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.0727272778749466)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.08181818574666977)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.09090909361839294)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.10000000149011612)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): Identity()\n",
       "    (pooler): BeitPooler(\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): MBartForCausalLM(\n",
       "    (model): MBartDecoderWrapper(\n",
       "      (decoder): MBartDecoder(\n",
       "        (embed_tokens): MBartScaledWordEmbedding(57528, 1024, padding_idx=1)\n",
       "        (embed_positions): MBartLearnedPositionalEmbedding(1538, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x MBartDecoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=57528, bias=False)\n",
       "  )\n",
       "  (enc_to_dec_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de9a1b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trainable: encoder.embeddings.cls_token\n",
      "✅ Trainable: encoder.embeddings.position_embeddings\n",
      "✅ Trainable: encoder.embeddings.patch_embeddings.projection.weight\n",
      "✅ Trainable: encoder.embeddings.patch_embeddings.projection.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.0.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.0.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.0.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.1.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.1.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.1.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.2.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.2.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.2.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.3.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.3.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.3.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.4.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.4.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.4.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.5.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.5.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.5.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.6.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.6.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.6.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.7.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.7.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.7.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.8.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.8.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.8.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.9.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.9.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.9.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.10.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.10.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.10.layernorm_after.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.lambda_1\n",
      "✅ Trainable: encoder.encoder.layer.11.lambda_2\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.attention.query.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.attention.query.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.attention.key.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.attention.value.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.attention.value.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.attention.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.intermediate.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.intermediate.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.output.dense.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.output.dense.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.layernorm_before.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.layernorm_before.bias\n",
      "✅ Trainable: encoder.encoder.layer.11.layernorm_after.weight\n",
      "✅ Trainable: encoder.encoder.layer.11.layernorm_after.bias\n",
      "✅ Trainable: encoder.pooler.layernorm.weight\n",
      "✅ Trainable: encoder.pooler.layernorm.bias\n",
      "✅ Trainable: decoder.model.decoder.embed_tokens.weight\n",
      "✅ Trainable: decoder.model.decoder.embed_positions.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.self_attn.k_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.self_attn.k_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.self_attn.v_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.self_attn.v_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.self_attn.q_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.self_attn.q_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.self_attn.out_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.self_attn.out_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.self_attn_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.self_attn_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.encoder_attn.k_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.encoder_attn.k_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.encoder_attn.v_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.encoder_attn.v_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.encoder_attn.q_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.encoder_attn.q_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.encoder_attn.out_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.encoder_attn.out_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.fc1.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.fc1.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.fc2.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.fc2.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.0.final_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.0.final_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.self_attn.k_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.self_attn.k_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.self_attn.v_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.self_attn.v_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.self_attn.q_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.self_attn.q_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.self_attn.out_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.self_attn.out_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.self_attn_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.self_attn_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.encoder_attn.k_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.encoder_attn.k_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.encoder_attn.v_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.encoder_attn.v_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.encoder_attn.q_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.encoder_attn.q_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.encoder_attn.out_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.encoder_attn.out_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.fc1.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.fc1.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.fc2.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.fc2.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.1.final_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.1.final_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.self_attn.k_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.self_attn.k_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.self_attn.v_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.self_attn.v_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.self_attn.q_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.self_attn.q_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.self_attn.out_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.self_attn.out_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.self_attn_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.self_attn_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.encoder_attn.k_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.encoder_attn.k_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.encoder_attn.v_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.encoder_attn.v_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.encoder_attn.q_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.encoder_attn.q_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.encoder_attn.out_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.encoder_attn.out_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.fc1.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.fc1.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.fc2.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.fc2.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.2.final_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.2.final_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.self_attn.k_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.self_attn.k_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.self_attn.v_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.self_attn.v_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.self_attn.q_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.self_attn.q_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.self_attn.out_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.self_attn.out_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.self_attn_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.self_attn_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.encoder_attn.k_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.encoder_attn.k_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.encoder_attn.v_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.encoder_attn.v_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.encoder_attn.q_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.encoder_attn.q_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.encoder_attn.out_proj.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.encoder_attn.out_proj.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.fc1.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.fc1.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.fc2.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.fc2.bias\n",
      "✅ Trainable: decoder.model.decoder.layers.3.final_layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layers.3.final_layer_norm.bias\n",
      "✅ Trainable: decoder.model.decoder.layernorm_embedding.weight\n",
      "✅ Trainable: decoder.model.decoder.layernorm_embedding.bias\n",
      "✅ Trainable: decoder.model.decoder.layer_norm.weight\n",
      "✅ Trainable: decoder.model.decoder.layer_norm.bias\n",
      "✅ Trainable: enc_to_dec_proj.weight\n",
      "✅ Trainable: enc_to_dec_proj.bias\n"
     ]
    }
   ],
   "source": [
    "ovmodel.config.max_length = max_token_size\n",
    "ovmodel.config.decoder.max_length = max_token_size\n",
    "# dropout = 0.2\n",
    "ovmodel.config.min_length = 1\n",
    "ovmodel.config.decoder.min_length = 1\n",
    "ovmodel.config.no_repeat_ngram_size = 0\n",
    "ovmodel.config.repetition_penalty = 1.2\n",
    "ovmodel.config.length_penalty = 1.0 \n",
    "ovmodel.config.early_stopping = True\n",
    "ovmodel.config.num_beams = 6\n",
    "ovmodel.config.use_cache = False  \n",
    "ovmodel.config.is_encoder_decoder = True\n",
    "ovmodel.config.do_sample = False  \n",
    "ovmodel.config.tie_word_embeddings = True\n",
    "print_trainable_prams(ovmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "506c25f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovmodel.config.decoder.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46037f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d11cabfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57528, 57528, 57528)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovmodel.config.vocab_size, ovmodel.config.decoder.vocab_size, len(text_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dac76e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaTokenizerFast(name_or_path='naver-clova-ix/donut-base', vocab_size=57522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<s_iitcdip>', '<s_synthdog>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57521: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t57522: AddedToken(\"<sep/>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57523: AddedToken(\"<s_iitcdip>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57524: AddedToken(\"<s_synthdog>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57525: AddedToken(\"Ã\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57526: AddedToken(\"Ê\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57527: AddedToken(\"Â\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42340f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1056"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovmodel.config.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea9a1f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57528, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(ovmodel.decoder.get_input_embeddings().weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "659c2c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=57528, bias=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovmodel.decoder.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1adb69e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaabi\\Documents\\comudel\\ocr\\func_utils\\trainer_utils.py:226: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10, \n",
    ")\n",
    "trainer = setup_dit_bart_training(\n",
    "        train_synthdataset, val_synthdataset, training_args=training_args, model=ovmodel, text_tokenizer=text_tokenizer,\n",
    "        run_name = run_name, \n",
    "        callbacks=[early_stopping_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8db7756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 0}.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1077' max='6300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1077/6300 15:10 < 1:13:41, 1.18 it/s, Epoch 17.08/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Pred Intersect Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.319000</td>\n",
       "      <td>9.819220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.484200</td>\n",
       "      <td>7.685655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.843500</td>\n",
       "      <td>7.306023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.631500</td>\n",
       "      <td>7.308141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.586500</td>\n",
       "      <td>7.191164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.450100</td>\n",
       "      <td>7.352375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.187800</td>\n",
       "      <td>7.256555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.273300</td>\n",
       "      <td>7.181708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.191000</td>\n",
       "      <td>7.287179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.174700</td>\n",
       "      <td>7.250978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>6.156900</td>\n",
       "      <td>7.209079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>6.041800</td>\n",
       "      <td>7.107923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>6.023400</td>\n",
       "      <td>7.180508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>6.041100</td>\n",
       "      <td>7.300870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.933900</td>\n",
       "      <td>7.280032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>5.973800</td>\n",
       "      <td>7.317741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>5.872700</td>\n",
       "      <td>7.288741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4034: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 1056, 'min_length': 1, 'early_stopping': True, 'num_beams': 6, 'repetition_penalty': 1.2}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\trainer.py:2623\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2621\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2622\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2623\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2625\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2626\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\trainer.py:5581\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5579\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5580\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5581\u001b[39m         batch_samples.append(\u001b[38;5;28mnext\u001b[39m(epoch_iterator))\n\u001b[32m   5582\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5583\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\accelerate\\data_loader.py:579\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m     current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\func_utils\\pydataloader.py:282\u001b[39m, in \u001b[36mSynthDogDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    280\u001b[39m     image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     image = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_tokenizer:\n\u001b[32m    285\u001b[39m     image_features = \u001b[38;5;28mself\u001b[39m.image_feature_extractor(image, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).pixel_values.squeeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21a2e675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: des des des\n",
      "Ground Truth: outras pessoas, sejam materiais ou corporais. O pagamento poderá ser feito diretamente ao terceiro\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "ovmodel.eval()\n",
    "vind = np.random.randint(0, len(train_synthdataset))\n",
    "sample = train_synthdataset[vind]  \n",
    "inputs = sample[\"pixel_values\"].unsqueeze(0).to(ovmodel.device)\n",
    "image = sample['image']\n",
    "text = sample['text']\n",
    "output_ids = ovmodel.generate(inputs, max_length=100, num_beams=6)\n",
    "prediction = text_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Ground Truth:\", sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3f5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'called the \"n\"-isomer . However the'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, AutoImageProcessor\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "tmodel = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "tmodel.eval()\n",
    "vind = np.random.randint(0, len(train_synthdataset))\n",
    "sample = train_synthdataset[vind]  \n",
    "inputs = processor(sample['image'],return_tensors=\"pt\").pixel_values.to(tmodel.device)\n",
    "image = sample['image']\n",
    "text = sample['text']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd5f731",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m plt.imshow(\u001b[43mimage\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea4e708c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "Ground Truth: called the \"n\"-isomer . However the\n"
     ]
    }
   ],
   "source": [
    "output_ids = tmodel.generate(inputs, max_length=100, num_beams=6)\n",
    "prediction = text_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Ground Truth:\", sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75aabc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: thess,\n",
      "Ground Truth: called the \"n\"-isomer . However the\n"
     ]
    }
   ],
   "source": [
    "inputs = sample[\"pixel_values\"].unsqueeze(0).to(ovmodel.device)\n",
    "output_ids = ovmodel.generate(inputs, max_length=100, num_beams=6)\n",
    "prediction = text_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Ground Truth:\", sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ae205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
