{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b56bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeasted90\u001b[0m (\u001b[33mbeasted90-comudel\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "from func_utils.plot_utils import show_image\n",
    "import matplotlib.pyplot as plt \n",
    "from glob import glob\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "import os \n",
    "\n",
    "import torch \n",
    "from func_utils.pydataloader import SynthDogDataset\n",
    "from func_utils.trainer_utils import *\n",
    "from encoder_decoder_model import (\n",
    "    init_dit_mbert_models_fixed, init_dit_dbart_models, \n",
    "    print_model_layer_sizes, load_pretrained_enc_dec_model, load_pretrained_iprocessor_tokenizer\n",
    "    )\n",
    "\n",
    "from func_utils.trainer_utils import unfreeze_last_n_encoder\n",
    "from transformers import TrainingArguments, Seq2SeqTrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "wandb.login()\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e4de073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# Load quantized model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "model_name = \"facebook/Perception-LM-1B\"\n",
    "processor = AutoProcessor.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ff7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7c4d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>ÁGUA É ESSENCIAL PARA A COMPREENSÃO E AÇÃO; CÂNCER, ÓRGÃOS, EMOÇÃO, TÊM INFLUÊNCIA, E ÍNDICES MOSTRAM EVOLUÇÃO.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'ÁGUA É ESSENCIAL PARA A COMPREENSÃO E AÇÃO; CÂNCER, ÓRGÃOS, EMOÇÃO, TÊM INFLUÊNCIA, E ÍNDICES MOSTRAM EVOLUÇÃO.'\n",
    "processor.decode(processor(text=text).input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d385710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|begin_of_text|>',\n",
       " 'eos_token': '<|eot_id|>',\n",
       " 'pad_token': '<|end_of_text|>',\n",
       " 'image_token': '<|image|>',\n",
       " 'video_token': '<|video|>'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ae8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # self-attention\n",
    "        # \"gate_proj\", \"up_proj\", \"down_proj\"      # MLP\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc7d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.cls_token\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.pos_embed\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.patch_embed.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.norm_pre.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.norm_pre.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.0.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.1.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.2.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.3.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.4.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.5.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.6.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.7.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.8.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.9.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.10.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.11.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.12.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.13.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.14.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.15.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.16.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.17.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.18.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.19.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.20.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.21.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.gamma_1\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.gamma_2\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.norm1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.norm1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.attn.qkv.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.attn.qkv.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.attn.proj.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.attn.proj.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.norm2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.norm2.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.mlp.fc1.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.mlp.fc1.bias\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.mlp.fc2.weight\n",
      "⛔ Frozen: base_model.model.model.vision_tower.timm_model.blocks.22.mlp.fc2.bias\n",
      "⛔ Frozen: base_model.model.model.multi_modal_projector.linear_1.weight\n",
      "⛔ Frozen: base_model.model.model.multi_modal_projector.linear_1.bias\n",
      "⛔ Frozen: base_model.model.model.multi_modal_projector.linear_2.weight\n",
      "⛔ Frozen: base_model.model.model.multi_modal_projector.linear_2.bias\n",
      "⛔ Frozen: base_model.model.model.language_model.embed_tokens.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.0.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.0.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.0.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.0.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.0.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.1.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.1.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.1.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.1.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.1.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.2.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.2.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.2.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.2.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.2.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.3.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.3.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.3.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.3.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.3.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.4.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.4.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.4.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.4.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.4.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.5.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.5.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.5.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.5.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.5.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.6.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.6.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.6.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.6.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.6.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.7.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.7.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.7.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.7.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.7.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.8.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.8.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.8.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.8.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.8.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.9.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.9.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.9.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.9.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.9.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.10.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.10.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.10.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.10.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.10.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.11.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.11.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.11.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.11.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.11.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.12.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.12.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.12.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.12.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.12.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.13.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.13.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.13.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.13.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.13.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.14.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.14.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.14.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.14.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.14.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "✅ Trainable: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.15.mlp.gate_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.15.mlp.up_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.15.mlp.down_proj.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.15.input_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.layers.15.post_attention_layernorm.weight\n",
      "⛔ Frozen: base_model.model.model.language_model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "print_trainable_prams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7472955a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jaabi\\Documents\\comudel\\ocr\\wandb\\run-20251009_122149-wimffi1t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/wimffi1t' target=\"_blank\">overfit_testing_plm</a></strong> to <a href='https://wandb.ai/beasted90-comudel/ocr%20model%20' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/beasted90-comudel/ocr%20model%20' target=\"_blank\">https://wandb.ai/beasted90-comudel/ocr%20model%20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/wimffi1t' target=\"_blank\">https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/wimffi1t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/beasted90-comudel/ocr%20model%20/runs/wimffi1t?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x21dc0bb9090>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name = \"overfit_testing_plm\"\n",
    "wandb.init(project=\"ocr model\", name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b846ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model as is is holding: 2.10 of GPU RAM\n",
      "['synthdog\\\\outputs\\\\SynthDoG-en\\\\train\\\\image_0.jpg', 'synthdog\\\\outputs\\\\SynthDoG-en\\\\train\\\\image_1.jpg']\n",
      "Sampled lang counter: {'SynthDoG-en': 16, 'SynthDoG-pt': 16}\n",
      "Length of _.images: 32 | Length of _.json_metadata: 91\n",
      "['synthdog\\\\outputs\\\\SynthDoG-en\\\\validation\\\\image_43.jpg']\n",
      "Sampled lang counter: {'SynthDoG-en': 1}\n",
      "Length of _.images: 1 | Length of _.json_metadata: 1\n"
     ]
    }
   ],
   "source": [
    "def get_synth_images_json_path(data_root= os.path.join('synthdog','outputs'), split='train'):\n",
    "    ipath = os.path.join(data_root, '*', split, '*.jpg')\n",
    "    json_path = os.path.join(data_root, '*', split, 'metadata.jsonl')\n",
    "\n",
    "    return glob(ipath), glob(json_path)\n",
    "\n",
    "\n",
    "root_path = os.path.join('synthdog', 'outputs')\n",
    "train_ipath, train_json_metadata = get_synth_images_json_path(data_root=root_path, split='train')\n",
    "val_ipath, val_json_metadata = get_synth_images_json_path(data_root=root_path, split='validation')\n",
    "\n",
    "peak_mem = torch.cuda.max_memory_allocated()\n",
    "print(f\"The model as is is holding: {peak_mem / 1024**3:.2f} of GPU RAM\")\n",
    "\n",
    "max_token_size = 1056\n",
    "sample_size = 32\n",
    "train_synthdataset = SynthDogDataset(train_ipath, train_json_metadata, image_feature_extractor=processor, \n",
    "                                     text_tokenizer=None, max_token_size=max_token_size, sample_size=sample_size)\n",
    "val_synthdataset = SynthDogDataset(val_ipath, val_json_metadata, image_feature_extractor=processor, \n",
    "                                   text_tokenizer=None, max_token_size=max_token_size, sample_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0fabd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionLMConfig {\n",
       "  \"architectures\": [\n",
       "    \"PerceptionLMForConditionalGeneration\"\n",
       "  ],\n",
       "  \"dtype\": \"float16\",\n",
       "  \"image_token_id\": 128002,\n",
       "  \"model_type\": \"perception_lm\",\n",
       "  \"projector_pooling_ratio\": 2,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": true,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"text_config\": {\n",
       "    \"attention_bias\": false,\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"bos_token_id\": 128000,\n",
       "    \"dtype\": \"float16\",\n",
       "    \"eos_token_id\": [\n",
       "      128001,\n",
       "      128009\n",
       "    ],\n",
       "    \"head_dim\": 64,\n",
       "    \"hidden_act\": \"silu\",\n",
       "    \"hidden_size\": 2048,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 8192,\n",
       "    \"max_position_embeddings\": 11520,\n",
       "    \"mlp_bias\": false,\n",
       "    \"model_type\": \"llama\",\n",
       "    \"num_attention_heads\": 32,\n",
       "    \"num_hidden_layers\": 16,\n",
       "    \"num_key_value_heads\": 8,\n",
       "    \"pretraining_tp\": 1,\n",
       "    \"rms_norm_eps\": 1e-05,\n",
       "    \"rope_scaling\": {\n",
       "      \"factor\": 32.0,\n",
       "      \"high_freq_factor\": 4.0,\n",
       "      \"low_freq_factor\": 1.0,\n",
       "      \"original_max_position_embeddings\": 8192,\n",
       "      \"rope_type\": \"llama3\"\n",
       "    },\n",
       "    \"rope_theta\": 500000.0,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 128256\n",
       "  },\n",
       "  \"transformers_version\": \"4.57.0\",\n",
       "  \"video_token_id\": 128003,\n",
       "  \"vision_config\": {\n",
       "    \"architecture\": \"vit_pe_core_large_patch14_336\",\n",
       "    \"do_pooling\": true,\n",
       "    \"dtype\": \"float16\",\n",
       "    \"global_pool\": \"map\",\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"label_names\": [\n",
       "      \"LABEL_0\",\n",
       "      \"LABEL_1\"\n",
       "    ],\n",
       "    \"model_args\": {\n",
       "      \"depth\": 23,\n",
       "      \"embed_dim\": 1024,\n",
       "      \"global_pool\": \"\",\n",
       "      \"img_size\": [\n",
       "        448,\n",
       "        448\n",
       "      ],\n",
       "      \"init_values\": 0.1,\n",
       "      \"ref_feat_shape\": [\n",
       "        32,\n",
       "        32\n",
       "      ],\n",
       "      \"use_post_transformer_norm\": false\n",
       "    },\n",
       "    \"model_type\": \"timm_wrapper\",\n",
       "    \"num_classes\": 2,\n",
       "    \"num_features\": 1024,\n",
       "    \"pretrained_cfg\": {\n",
       "      \"classifier\": \"head\",\n",
       "      \"crop_mode\": \"center\",\n",
       "      \"crop_pct\": 1.0,\n",
       "      \"custom_load\": false,\n",
       "      \"first_conv\": \"patch_embed.proj\",\n",
       "      \"fixed_input_size\": true,\n",
       "      \"input_size\": [\n",
       "        3,\n",
       "        336,\n",
       "        336\n",
       "      ],\n",
       "      \"interpolation\": \"bicubic\",\n",
       "      \"license\": \"custom\",\n",
       "      \"mean\": [\n",
       "        0.5,\n",
       "        0.5,\n",
       "        0.5\n",
       "      ],\n",
       "      \"pool_size\": null,\n",
       "      \"std\": [\n",
       "        0.5,\n",
       "        0.5,\n",
       "        0.5\n",
       "      ],\n",
       "      \"tag\": \"fb\"\n",
       "    }\n",
       "  },\n",
       "  \"vision_use_cls_token\": true\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c8920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"./{run_name}\",\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=5e-5,  \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        warmup_ratio=0.1,  \n",
    "        logging_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        save_total_limit=3,\n",
    "        bf16=True,\n",
    "        max_grad_norm=1.0,  \n",
    "        weight_decay=0.01,\n",
    "        optim='adamw_torch',\n",
    "        dataloader_pin_memory=True,\n",
    "        # dataloader_num_workers=2,\n",
    "        prediction_loss_only=True,\n",
    "        \n",
    "        report_to=[\"wandb\"],\n",
    "        run_name=run_name,\n",
    "\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        load_best_model_at_end=True,  \n",
    "        greater_is_better=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02c5d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plm_collate_fn(batch, processor):\n",
    "    \"\"\"\n",
    "    Collate function for Perception-LM fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of dicts with 'pixel_values' (PIL Image or tensor) and 'labels' (str)\n",
    "        processor: AutoProcessor for Perception-LM\n",
    "    \n",
    "    Returns:\n",
    "        Dict with processed inputs ready for the model\n",
    "    \"\"\"\n",
    "    # Extract images and text labels\n",
    "    images = [item['pixel_values'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Process images - processor handles PIL Images or tensors\n",
    "    # Returns pixel_values tensor\n",
    "    inputs = processor(\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Process text labels\n",
    "    # For Perception-LM, we need to tokenize the target text\n",
    "    text_inputs = processor.tokenizer(\n",
    "        labels,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512  # Adjust based on your label lengths\n",
    "    )\n",
    "    \n",
    "    # The model expects 'labels' for the decoder during training\n",
    "    # Replace padding token ids with -100 so they're ignored in loss\n",
    "    labels_tensor = text_inputs['input_ids'].clone()\n",
    "    labels_tensor[labels_tensor == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Combine everything\n",
    "    inputs['labels'] = labels_tensor\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "def collate_fn_with_prompt(batch, max_length =1056, pad_token_id=processor.tokenizer.pad_token_id):\n",
    "    \"\"\"\n",
    "    Collates already-processed outputs with instruction, [IMG], image tensors, and label token ids.\n",
    "    Does not re-tokenize prompt—just pads and batches.\n",
    "    \"\"\"\n",
    "    # Each item: {'pixel_values', 'input_ids', 'attention_mask', 'labels', ...}\n",
    "    pixel_values = []\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for item in batch:\n",
    "        pixel_values.append(item['pixel_values'])\n",
    "        input_ids.append(item['input_ids'])\n",
    "        attention_mask.append(item['attention_mask'])\n",
    "        labels.append(item['labels'])\n",
    "\n",
    "    # Pad sequences (input_ids, attention_mask, labels) to max length in batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n",
    "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "    # Mask labels padding token for loss computation\n",
    "    labels_for_loss = labels_padded.clone()\n",
    "    labels_for_loss[labels_padded == pad_token_id] = -100\n",
    "\n",
    "    # Stack pixel_values if tensor shape matches (else use torch.cat or list as needed)\n",
    "    pixel_values_batch = torch.stack(pixel_values)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_mask_padded,\n",
    "        'pixel_values': pixel_values_batch,\n",
    "        'labels': labels_for_loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b601b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=5, \n",
    ")\n",
    "# trainer = setup_dit_bart_training(\n",
    "#         train_synthdataset, val_synthdataset, training_args=training_args, model=model, text_tokenizer=processor.tokenizer,\n",
    "#         run_name = run_name, \n",
    "#         callbacks=[early_stopping_callback],\n",
    "#         max_length=max_token_size,\n",
    "#         custom_collate_fn=collate_fn_with_prompt\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2706c8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is holding: 2.10 of GPU RAM\n"
     ]
    }
   ],
   "source": [
    "peak_mem = torch.cuda.max_memory_allocated()\n",
    "print(f\"The model is holding: {peak_mem / 1024**3:.2f} of GPU RAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08a06d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaabi\\AppData\\Local\\Temp\\ipykernel_9508\\1327937878.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# collate_fn = lambda batch: collate_fn_with_prompt(batch, processor.tokenizer)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_synthdataset,\n",
    "        eval_dataset=train_synthdataset,\n",
    "        data_collator=collate_fn_with_prompt,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        # compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0419979",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 18144, 8988) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1284\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\queue.py:179\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    180\u001b[39m \u001b[38;5;28mself\u001b[39m.not_empty.wait(remaining)\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\transformers\\trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\accelerate\\data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1492\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1491\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1495\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaabi\\Documents\\comudel\\ocr\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1298\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1297\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1299\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1300\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 18144, 8988) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32844a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Extract all the texts from the document.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(processor(images=train_synthdataset[0]['image'], text = \"Extract all the texts from the document.\").input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e51762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
